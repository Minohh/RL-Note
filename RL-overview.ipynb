{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties of Reinforcement Learning\n",
    "\n",
    "1. Reward delay\n",
    "2. Agent's action affect the subsequent data it receives\n",
    "\n",
    "\n",
    "# Method categories\n",
    "\n",
    "1. Policy-based. Learning an actor.\n",
    "2. Value-based. Learning a critic.\n",
    "3. Actor + Critic.Popular method: Asynchronous Advantage Actor-Critic (A3C).\n",
    "\n",
    "# Learn an actor\n",
    "\n",
    "Actor/Policy\n",
    "\n",
    "$$Action = \\pi(observation)$$\n",
    "\n",
    "Three steps \n",
    "1. Define a set of function (Neural Network as Actor).\n",
    "2. Goodness of function.\n",
    "3. Pick the best function.\n",
    "\n",
    "\n",
    "### Neural network as Actor\n",
    "\n",
    "- Structure: Neural network\n",
    "- Input: observatinon (image pixels etc.)\n",
    "- Output: Actions (each action corresponds to a neuron in output layer), the probability of taking the actions.\n",
    "\n",
    "The benefit of using network instead of lookup table is it can handle the unseen abservations.\n",
    "\n",
    "\n",
    "### Goodness of actor.\n",
    "\n",
    "- Given an actor $\\pi_{\\theta}(s)$ with network parameter $\\theta$.\n",
    "- Use the actor $\\pi_{\\theta}(s)$ to play the video game.\n",
    "\n",
    "Episode:\n",
    "\n",
    "- Start with observation $s_1$\n",
    "- Machine decides to take $a_1$\n",
    "- Machine obtains reward $r_1$\n",
    "- Machine sees observation $s_2$\n",
    "- Machine decides to take $a_2$\n",
    "- Machine obtains reward $r_2$\n",
    "- Machine sees observation $s_3$\n",
    "- ......\n",
    "- Machine decides to take $a_T$\n",
    "- Machine obtains reward $r_T$\n",
    "\n",
    "Total reward: $R_{\\theta} = \\sum_{t=1}^T r_t$.\n",
    "\n",
    "However, even with the same actor, $R_{\\theta}$ is different each time for the randomness in the actor and the game. Hence, our goal is to obtain the largest $\\bar{R}_{\\theta}$, which is the expected value of $R_{\\theta}$ and evaluates the goodness of an actor $\\pi_{\\theta}(s)$.\n",
    "\n",
    "We used a trajectory $\\tau$ to represent an episode\n",
    "\n",
    "$$\\tau = \\{s_1, a_1, r_1, s_2, a_2, r_2,\\cdots,s_T, a_T, r_T\\}$$\n",
    "\n",
    "the total reward is \n",
    "\n",
    "$$R(\\tau) = \\sum_{t=1}^T r_t$$\n",
    "\n",
    "If you use an actor to play the game, each $\\tau$ has a probability to be sampled. The probability depends on actor parameter $\\theta$, thus $P(\\tau|\\theta)$. The trajectory $\\tau$ can be interpreted as a gaming process, $P(\\tau|\\theta)$ represents the probability of appearing of the gaming process given the actor that is governed by $\\theta$. And gaming process consists of rewards. Our goal is to obtain the higher expected reward by adjusting the actor ($\\theta$).\n",
    "\n",
    "$$\\bar{R}_{\\theta} = \\sum_{\\tau} R(\\tau)P(\\tau|\\theta)$$\n",
    "\n",
    "Practically, we can use $\\pi_{\\theta}$ to play the game $N$ times to obtain $\\{\\tau^1, \\tau^2, \\cdots, \\tau^N\\}$, which can be view as samples that sampling from $P(\\tau|\\theta)$ $N$ times. Then the expected total reward is \n",
    "\n",
    "$$\\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)$$\n",
    "\n",
    "### Pick the best function\n",
    "\n",
    "$$\\theta^* = arg \\underset{\\theta}{\\text{max}} \\bar{R}_{\\theta}$$\n",
    "\n",
    "Gradient ascent\n",
    "\n",
    "- start with $\\theta^0$\n",
    "- $\\theta^1 \\leftarrow \\theta^0 + \\eta \\nabla \\bar{R}_{\\theta^0}$\n",
    "- $\\theta^2 \\leftarrow \\theta^1 + \\eta \\nabla \\bar{R}_{\\theta^1}$\n",
    "- ......\n",
    "\n",
    "The gradient takes the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla\\bar{R}_{\\theta} &= \\nabla \\sum_{\\tau} R(\\tau)P(\\tau|\\theta)\\\\\n",
    "&= \\sum_{\\tau} R(\\tau) \\nabla P(\\tau|\\theta)\\qquad \\text{Thus, }R(\\tau)\\text{ do not have to be differentiable}\\\\\n",
    "&= \\sum_{\\tau} R(\\tau) P(\\tau|\\theta)\\frac{\\nabla P(\\tau|\\theta)}{P(\\tau|\\theta)}\\\\\n",
    "&= \\sum_{\\tau} R(\\tau) P(\\tau|\\theta)\\nabla log P(\\tau|\\theta) \\qquad \\frac{dlog(f(x))}{dx} = \\frac{1}{f(x)}\\frac{df(x)}{dx}\\\\\n",
    "&\\approx \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla log P(\\tau^n|\\theta) \\qquad \\text{N Samples}\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla log\\Big(p(s_1^n)p(a_1^n|s_1^n,\\theta)p(r_1^n, s_2^n|s_1^n, a_1^n)p(a_2^n|s_2^n,\\theta)p(r_2^n,s_3^n|s_2^n,a_2^n)\\cdots\\Big)\\qquad \\text{probability of the nth trajectory}\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla log\\Big(p(s_1^n)\\prod_{t=1}^T p(a_t^n|s_t^n,\\theta)p(r_t^n, s_{t+1}^n|s_t^n, a_t^n)\\Big)\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla \\Big(log p(s_1^n) + \\sum_{t=1}^T log p(a_t^n|s_t^n, \\theta)+\\sum_{t=1}^T log p(r_t^n, s_{t+1}^n|s_t^n, a_t^n)\\Big)\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N R(\\tau^n)\\nabla \\sum_{t=1}^T log p(a_t^n|s_t^n, \\theta)\\\\\n",
    "&= \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T R(\\tau^n)\\nabla log p(a_t^n|s_t^n, \\theta)\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "### Equation analyse\n",
    "\n",
    "The gradient of the expected reward $\\bar{R}_{\\theta}$ with respect to $\\theta$ is given by\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T R(\\tau^n)\\nabla log p(a_t^n|s_t^n, \\theta)$$\n",
    "\n",
    "We can draw several action picking preference of actor from this equation.\n",
    "\n",
    "1. The actor is clined to increase the probability of the action that gives the positive reward, whereas decrease the probability of the action that gives the negative reward.  \n",
    "Extremely, by assuming the $\\theta$ only control one action of a given state in a given trajactory, we transform the expected reward function to the following form\n",
    "$$\\bar{R}_{\\theta} = \\sum_{\\tau } R(\\tau)P(\\tau|\\theta) = \\color{#cccccc}{\\sum_{\\tau \\neq \\tau^j} R(\\tau)P(\\tau|\\theta) + \\Big(p(s_1^j)\\prod_{t\\neq i}p(a_t^j|s_t^j)\\prod_{t}p(r_t^j, s_{t+1}^n|s_t^j, a_t^j) \\Big)}R(\\tau^j)p(a_i^j|s_i^j, \\theta) $$\n",
    "The gradient ascent always helps us obtain higher reward $\\bar{R}_{\\theta}$. Moreover, trajectory reward $R(\\tau^j)$ is a constant. Thus\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{If }R(\\tau^j) \\text{ is positive, }p(a_i^j|s_i^j) \\text{ will increase}\\\\\n",
    "\\text{If }R(\\tau^j) \\text{ is negative, }p(a_i^j|s_i^j) \\text{ will decrease}\\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "2. The actor prefers the actions that gives higher reward, i.e. increase the probability, which is guaranteed by the logarithm form.\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T R(\\tau^n)\\nabla log p(a_t^n|s_t^n, \\theta) = \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T R(\\tau^n) \\frac{\\nabla p(a_t^n|s_t^n, \\theta)}{p(a_t^n|s_t^n, \\theta)}$$\n",
    "The denominator is regarded the normalization term such that eliminate the affect of the probability and increase the affect of the rewards.\n",
    "\n",
    "\n",
    "### Add a baseline\n",
    "\n",
    "In the enviromment that all the rewards are positive, the quantity of the probability of each action will increase, which is not the issue, because the probability is summed up to 1 such that the probability of the action that adds less quantity would decrease. However, practially, the probability are derived from sample data and there would exist the case that an action with high probability are not sampled, which would lead to improper probability decline over such actions.\n",
    "\n",
    "To address this issue, a baseline that minus by the reward is introduced.\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T (R(\\tau^n)-b)\\nabla log p(a_t^n|s_t^n, \\theta)$$\n",
    "\n",
    "The baseline helps alleviate the problem of probability decline over unsampled high reward actions.\n",
    "\n",
    "\n",
    "### Assign Suitable Credit\n",
    "\n",
    "The action does not contributes to the former rewards but to the subsequent accumulated reward.\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T \\left(\\bbox[yellow]{\\sum_{t'=t}^T r_{t'}^n}-b\\right)\\nabla log p(a_t^n|s_t^n, \\theta)$$\n",
    "\n",
    "Further away, the affect is weaker (add a discount).\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T \\underbrace{\\left(\\bbox[yellow]{\\sum_{t'=t}^T \\gamma^{t'-t}r_{t'}^n}-b\\right)}_{\\text{Advantage Function } A^{\\theta}(s_t, a_t)}\\nabla log p(a_t^n|s_t^n, \\theta)$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Alternative perspective\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N \\sum_{t=1}^T \\underbrace{R(\\tau^n)}_{\\text{weight}}\\underbrace{\\nabla log p(a_t^n|s_t^n, \\theta)}_{\\text{classification}}$$\n",
    "\n",
    "\n",
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "### On-policy v.s. Off-policy\n",
    "\n",
    "- **On-policy**. Learn by playing game yourself.  \n",
    "Conventionally, the policy gradient learning method has the following steps:  \n",
    "  1. Use an agent to play a game $N$ times which correponds $N$ trajactories.\n",
    "  2. The enviromment and the agent will also generate $N$ total rewards.\n",
    "  3. For each time step in each trajectory, use their temporal state, action as well as the rewards to train the agent network.\n",
    "  4. Use the gradient, which is the accumulated gradient of the N trajectories to update the network.  \n",
    "  \n",
    "  An update of network needs to play the game $N$ times, and after the network update, we have to play the game (sample the training data) again, which is too inefficient.\n",
    "  \n",
    "- **Off-policy**. Learn by watching others playing the game (or the archive video).  \n",
    "The goal of off-policy is to use the sample which are sampled from $\\pi_{\\theta'}$ to train our current network coefficient $\\theta$. $\\theta'$ is fixed, so we can re-use the sample data.\n",
    "\n",
    "### Importance Sampling\n",
    "\n",
    "- Target: evaluate the expectation of $f(x)$ where $x$ derives from the distribution $p(x)$.\n",
    "$$E_{x \\sim p}[f(x)] = \\int f(x)p(x) dx$$\n",
    "- Approximation: \n",
    "$$E_{x \\sim p}[f(x)]\\approx \\frac{1}{N}\\sum_{i=1}^N f(x^i)$$\n",
    "- If we only have $x^i$ sampled from distribution $q(x)$ instead of distribution $p(x)$\n",
    "$$E_{x \\sim p}[f(x)] = \\int f(x)\\frac{p(x)}{q(x)}q(x) dx = E_{x \\sim q}\\left[f(x)\\frac{p(x)}{q(x)}\\right]$$\n",
    "Notice that the variance of the original and the current expectation is different.\n",
    "$$\\begin{array}{ll}\n",
    "Var[X] &= E[X^2] &- &(E[X])^2\\\\\n",
    "Var_{x\\sim p}[f(x)] &= E_{x\\sim p}[f(x)^2] &- &(E_{x\\sim p}[f(x)])^2\\\\\n",
    "Var_{x\\sim q}\\left[f(x)\\frac{p(x)}{q(x)}\\right] &= E_{x\\sim p}\\left[f(x)^2\\frac{p(x)}{q(x)}\\right] &- &(E_{x\\sim p}[f(x)])^2\\\\\n",
    "\\end{array}$$\n",
    "Consequently, in the case that the distribution $p$ and $q$ are fairly different, if our sample data is not enough, we may not obtain a similar expectation. As a result, it is better to constrain the distribution $p$ such that it is relatively close to the distribution $q$.\n",
    "\n",
    "### On-policy to Off-policy\n",
    "\n",
    "Using the inportance sampling method, the gradient of reward $\\nabla \\bar{R}_{\\theta}$ turns out to be able to sampled from the distribution $p_{\\theta'}(\\tau)$\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla \\bar{R}_{\\theta} &= E_{\\tau\\sim p_{\\theta}(\\tau)}\\left[R(\\tau) \\nabla log p_{\\theta}(\\tau)\\right]\\\\\n",
    "&= E_{\\tau\\sim p_{\\theta'}(\\tau)}\\left[\\frac{p_{\\theta}(\\tau)}{p_{\\theta'}(\\tau)}R(\\tau) \\nabla log p_{\\theta}(\\tau)\\right]\n",
    "\\end{align*}$$\n",
    "\n",
    "Advantage form\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla \\bar{R}_{\\theta} &= E_{(s_t, a_t)\\sim \\pi_{\\theta}}\\left[A^{\\theta}(s_t, a_t) \\nabla log p_{\\theta}(a_t^n|s_t^n)\\right]\\\\\n",
    "&= \\underbrace{E_{(s_t, a_t)\\sim \\pi_{\\theta'}}\\left[\\frac{p_{\\theta}(s_t, a_t)}{p_{\\theta'}(s_t, a_t)}\\bbox[yellow]{A^{\\theta}(s_t, a_t)} \\nabla log p_{\\theta}(a_t^n|s_t^n)\\right]}_{A^{\\theta}(s_t, a_t) \\text{ is actually } A^{\\theta'}(s_t, a_t)\\text{ because it's computed using the rewards from }\\pi_{\\theta'}}\\\\\n",
    "&= \\underbrace{E_{(s_t, a_t)\\sim \\pi_{\\theta'}}\\left[\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta'}(a_t|s_t)}\\bbox[yellow]{\\frac{p_{\\theta}(s_t)}{p_{\\theta'}(s_t)}}A^{\\theta}(s_t, a_t) \\nabla log p_{\\theta}(a_t^n|s_t^n)\\right]}_{\\text{eliminate }\\frac{p_{\\theta}(s_t)}{p_{\\theta'}(s_t)} \\text{by assuming the distribution of state has nothing to do with the agent}}\\\\\n",
    "&= E_{(s_t, a_t)\\sim \\pi_{\\theta'}}\\left[\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta'}(a_t|s_t)}A^{\\theta}(s_t, a_t) \\nabla log p_{\\theta}(a_t^n|s_t^n)\\right]\\\\\n",
    "&= \\nabla J^{\\theta'}(\\theta)\n",
    "\\end{align*}$$\n",
    "\n",
    "where $J^{\\theta'}(\\theta)$ is an objective function and takes the form\n",
    "\n",
    "$$J^{\\theta'}(\\theta) = E_{(s_t, a_t)\\sim \\pi_{\\theta'}}\\left[\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta'}(a_t|s_t)}A^{\\theta'}(s_t, a_t)\\right]$$\n",
    "\n",
    "which comes from the equation $\\nabla f(x) = f(x)\\nabla log f(x)$.\n",
    "\n",
    "### TRPO / PPO /  PPO2\n",
    "\n",
    "\n",
    "Recall that in the discussion of importance sampling, the distribution of $p$ and $q$ have to be similar in order to give similar expectation. Therefore while evaluating the gradient of the reward, the distribution of $\\pi_{\\theta}$ and $\\pi_{\\theta'}$ must be subjected to a constraint.\n",
    "\n",
    "In Trust Region Policy Optimization (TRPO), we use constraint to address this issue.\n",
    "$$J_{TRPO}^{\\theta'}(\\theta) = J^{\\theta'}(\\theta)\\quad \\text{ subject to } \\quad KL(\\theta, \\theta')<\\delta$$\n",
    "\n",
    "In Proximal Policy Optimization (PPO), we use lagrange multiplier to address this issue, which is easier-implemented and  than TRPO.\n",
    "$$J_{PPO}^{\\theta'}(\\theta) = J^{\\theta'}(\\theta) - \\beta KL(\\theta, \\theta')$$\n",
    "\n",
    "Both the two method use KL-divergence to measure the difference of distributions of $\\pi_{\\theta}$ and $\\pi_{\\theta'}$, i.e. $p_{\\theta}(a_t|s_t)$ and $p_{\\theta'}(a_t|s_t)$.\n",
    "\n",
    "In PPO2, the distribution is cliped for constrain.\n",
    "\n",
    "$$J_{PPO2}^{\\theta'}(\\theta) = E_{(s_t, a_t)\\sim \\pi_{\\theta'}}\\left[min\\left(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta'}(a_t|s_t)}A^{\\theta'}(s_t, a_t), clip\\left(\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta'}(a_t|s_t)}, 1-\\epsilon, 1+\\epsilon\\right)A^{\\theta'}(s_t, a_t)\\right)\\right]$$\n",
    "\n",
    "### PPO implementation\n",
    "\n",
    "1. Initial policy parameter $\\theta^0$.\n",
    "2. Using $\\theta^k$ to interact with the envirnment to collect ${s_t, a_t}$ and compute advantage $A^{\\theta^k}(s_t, a_t)$.\n",
    "3. Substitute ${s_t, a_t}$ and $A^{\\theta^k}(s_t, a_t)$ into the objective function $J_{PPO}^{\\theta^k}(\\theta)$, then use gradient ascent to optimize this function such that update $\\theta$.\n",
    "$$J_{PPO}^{\\theta^k}(\\theta) = J^{\\theta^k}(\\theta) - \\beta KL(\\theta, \\theta^k)\\qquad \\text{where}\\quad J^{\\theta^k}(\\theta)\\approx \\sum_{s_t, a_t}\\frac{p_{\\theta}(a_t|s_t)}{p_{\\theta^k}(a_t|s_t)}A^{\\theta^k}(s_t, a_t)$$\n",
    "4. Repeat step 3, i.e. update $\\theta$ several times.\n",
    "5. Run step 2 again.\n",
    "\n",
    "### Adaptive KL Penalty\n",
    "\n",
    "- If $KL(\\theta, \\theta^k)> KL_{max}$, increase $\\beta$.\n",
    "- If $KL(\\theta, \\theta^k)< KL_{min}$, decrease $\\beta$.\n",
    "\n",
    "\n",
    "# Learn a critic\n",
    "\n",
    "## State value function\n",
    "\n",
    "- A critic does not directly determine the action.\n",
    "- Given an actor $\\pi$, it evaluates how good the actor is.\n",
    "- State value function $V^{\\pi}(s)$, indicates when using an actor $\\pi$, the accumulated reward expects to be obtained after visiting state $s$.\n",
    "\n",
    "In a word, the critic evaluate the reward of actor when facing some state.\n",
    "\n",
    "### Method of evaluation\n",
    "\n",
    "1. **Monte-Carlo (MC) based approach**. Suppose when using an actor $pi$, after seeing $s$, until the end of the episode, the expected cumulated reward is $G$, then the critic $V^{\\pi}$, which is a network, should output $G$.\n",
    "$$\\begin{array}{ll}\n",
    "\\bbox[#aaaaff]{\\underbrace{\\Big[s_a\\Big]}_{\\text{input}}}\\rightarrow \\bbox[#ffaaaa]{\\underbrace{\\Big[V^{\\pi}\\Big]}_{\\text{Network}}}\\rightarrow V^{\\pi}(s_a)\\leftrightarrow \\bbox[#aaaaff]{\\underbrace{\\Big[G_a\\Big]}_{\\text{target}}}\\\\\n",
    "\\bbox[#aaaaff]{\\underbrace{\\Big[s_b\\Big]}_{\\text{input}}}\\rightarrow \\bbox[#ffaaaa]{\\underbrace{\\Big[V^{\\pi}\\Big]}_{\\text{Network}}}\\rightarrow V^{\\pi}(s_b)\\leftrightarrow \\bbox[#aaaaff]{\\underbrace{\\Big[G_b\\Big]}_{\\text{target}}}\\\\\n",
    "\\end{array}$$\n",
    "This approach need us provides the accumulated reward which can only be obtained after finishing game episode. However, some games have very long episodes, so that delaying all learning until an episode's end is too slow. TD can address such issue.\n",
    "2. **Temporal-difference (TD) approach**. The TD approach use the state of two subsequent states as input and the corresponding expected temporal-difference reward as output.\n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "\\bbox[#aaaaff]{\\underbrace{\\Big[s_t\\Big]}_{\\text{input}}} &\\rightarrow \\bbox[#ffaaaa]{\\underbrace{\\Big[V^{\\pi}\\Big]}_{\\text{Network}}} &\\rightarrow V^{\\pi}(s_t)\\\\\n",
    "\\bbox[#aaaaff]{\\underbrace{\\Big[s_{t+1}\\Big]}_{\\text{input}}} &\\rightarrow \\bbox[#ffaaaa]{\\underbrace{\\Big[V^{\\pi}\\Big]}_{\\text{Network}}} &\\rightarrow V^{\\pi}(s_{t+1})\\\\\n",
    "\\end{array}\\right\\} \n",
    "\\quad \\bbox[#dddddd]{\\Big[- \\Big]}\n",
    "\\rightarrow V^{\\pi}(s_t) - V^{\\pi}(s_{t+1})\n",
    "\\leftrightarrow \\bbox[#aaaaff]{\\underbrace{\\Big[r\\Big]}_{\\text{target}}}$$\n",
    "\n",
    "Obviously, learning a critic is a regression problem.\n",
    "\n",
    "### MC v.s. TD\n",
    "\n",
    "In MC, the target $G$ is actually a random variable which is composed of consecutive random variables correponding to various steps. The variance of $G$ is therefore very large.\n",
    "\n",
    "In TD, the temporal-difference reward $r$ is also a random variable. But it has smaller variance. However, $V^{\\pi}(s_{t+1})$ may be inaccurate.\n",
    "\n",
    "### Example\n",
    "\n",
    "- $s_a, r=0, s_b, r=0, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=1, $ END\n",
    "- $s_b, r=0, $ END\n",
    "\n",
    "In this episode. the expected reward after seeing $s_b$ is $V^{\\pi}(s_b) = 3/4$. The expected reward after seeing $s_a$ is \n",
    "- Monte-Carlo: $V^{\\pi}(s_a) = 0$\n",
    "- Temporal-difference: $V^{\\pi}(s_a) = V^{\\pi}(s_b) + r = 3/4$\n",
    "\n",
    "\n",
    "## State-action value function\n",
    "\n",
    "State-action value function which is denoted by $Q^{\\pi}(s, a)$, indicates that when using actor $\\pi$, the cumulated reward expects to be obtained after taking $a$ at state $s$.\n",
    "\n",
    "$$\n",
    "\\left.\\begin{array}{ll}\n",
    "\\bbox[#aaaaff]{\\big[s\\big]} &\\rightarrow \\\\\n",
    "\\bbox[#aaffaa]{\\big[a\\big]} &\\rightarrow \\\\\n",
    "\\end{array}\\right.\n",
    "\\underbrace{\\bbox[#ffaaaa]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{network}}\n",
    "\\rightarrow Q^{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "Now supposed that we are using the policy $\\pi$ and facing the state $s$. If we have a better policy $\\pi'$ that gives better critic than $\\pi$, then we can replace $\\pi$ with $\\pi'$ to achive better critic, i.e. higher reward. In other words, if we can train the policy network $\\pi$ to take the best action in order to achive higher reward, then our policy will become better. \n",
    "\n",
    "Mathematically, \"better\" is denoted by\n",
    "\n",
    "$$ \\begin{array}{ll}\n",
    "& &V^{\\pi'}(s) &\\geqslant &V^{\\pi}(s)\\qquad &\\text{for the spacific state }s\\\\\n",
    "&i.e. &Q^{\\pi'}(s, \\pi'(s)) &\\geqslant &Q^{\\pi}(s, \\pi(s))\\qquad &\\text{for the spacific state }s\n",
    "\\end{array}$$\n",
    "\n",
    "Practically, the network is trained using various sampled state, thus, by training the network $Q^{\\pi}(s, \\pi(s))$, we can get a better network $Q^{\\pi'}(s, \\pi'(s))$ for all state\n",
    "\n",
    "$$Q^{\\pi'}(s, \\pi'(s))\\geqslant Q^{\\pi}(s, \\pi(s))\\qquad \\text{for all state }s$$\n",
    "\n",
    "In the Q-Learning algorithm, when using policy $\\pi$, we want the policy change to always takes it best action that gives the maximal $Q$ value. At that point, the policy $\\pi$ has become another policy $\\pi'$.\n",
    "\n",
    "$$\\pi'(s) = arg \\underset{a}{\\text{max}}Q^{\\pi}(s, a)$$\n",
    "\n",
    "In the perspective of neural network, we have\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{Network:} & Q^{\\pi}(s, a)\\\\\n",
    "\\text{Input:} & s,a\\\\\n",
    "\\text{Target:} & \\underset{a}{\\text{max}}Q^{\\pi}(s, a)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "# Q-Learning Tips\n",
    "\n",
    "### Training using TD approach\n",
    "\n",
    "$$\n",
    "\\left.\\begin{array}{ll}\n",
    "\\bbox[#aaaaff]{\\big[s_t\\big]} &\\rightarrow \\\\\n",
    "\\bbox[#aaffaa]{\\big[a_t\\big]} &\\rightarrow \\\\\n",
    "\\end{array}\\right.\n",
    "\\underbrace{\\bbox[#ffaaaa]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{network}}\n",
    "\\rightarrow Q^{\\pi}(s_t, a_t)\n",
    "\\leftrightarrow \\bbox[#ccccff]{\\Big[r_t\\Big]} + Q^{\\pi}(s_{t+1}, \\pi(s_{t+1}))\n",
    "\\leftarrow \n",
    "\\underbrace{\\bbox[#ffaaaa]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{network}}\n",
    "\\left.\\begin{array}{ll}\n",
    "\\leftarrow &\\bbox[#aaaaff]{\\big[s_{t+1}\\big]}  \\\\\n",
    "\\leftarrow &\\bbox[#aaffaa]{\\big[\\pi(s_{t+1})\\big]} \\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Although we can train this two networks, which are actually the same, simultaneously, it will be unstable because the target change too frequently. As a result, it is more often to fix the network on the right-hand side.\n",
    "\n",
    "$$\n",
    "\\left.\\begin{array}{ll}\n",
    "\\bbox[#aaaaff]{\\big[s_t\\big]} &\\rightarrow \\\\\n",
    "\\bbox[#aaffaa]{\\big[a_t\\big]} &\\rightarrow \\\\\n",
    "\\end{array}\\right.\n",
    "\\underbrace{\\bbox[#ffaaaa]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{network}}\n",
    "\\rightarrow Q^{\\pi}(s_t, a_t)\n",
    "\\leftrightarrow \\underbrace{\\bbox[#ccccff]{\\Big[r_t + Q^{\\pi}(s_{t+1}, \\pi(s_{t+1}))\\Big]}}_{\\text{target}}\n",
    "\\leftarrow \n",
    "\\underbrace{\\bbox[#bbbbbb]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{fix}}\n",
    "\\left.\\begin{array}{ll}\n",
    "\\leftarrow &\\bbox[#aaaaff]{\\big[s_{t+1}\\big]}  \\\\\n",
    "\\leftarrow &\\bbox[#aaffaa]{\\big[\\pi(s_{t+1})\\big]} \\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "After updating the left-hand side network $N$ times, the right-hand side fixed network is replaced with the left-hand side network.\n",
    "\n",
    "\n",
    "\n",
    "### Exploration\n",
    "\n",
    "The Q function provide us the critic. If we only select the action that gives the highest critic, then the generated data, which will be used as the training data, is limited. E.g. suppose that the Q function is a not a network but a lookup table which is initialized to be zero. If at state s, the action $a_2$ is selected and the Q function returns 1, then $a_2$ will be always sampled because it gives the highest value.\n",
    "$$\n",
    "s\\left\\{\n",
    "\\begin{array}{ll}\n",
    "a_1 & Q(s, a) = 0 &\\text{Never explore}\\\\\n",
    "a_2 & Q(s, a) = 1 &\\text{Always sampled}\\\\\n",
    "a_3 & Q(s, a) = 0 &\\text{Never explore}\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "#### Epsilon Greedy\n",
    "\n",
    "$$a = \\left\\{\n",
    "\\begin{array}{ll}\n",
    "arg \\underset{a}{\\text{max}} Q(s, a) & \\text{with probability }1-\\epsilon\\\\\n",
    "random \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "where, $\\epsilon$ would decay during learning.\n",
    "\n",
    "#### Boltzmann Exploration\n",
    "\n",
    "$$\n",
    "p(a|s) = \\frac{exp(Q(s, a))}{\\sum_a exp(Q(s,a))}\n",
    "$$\n",
    "\n",
    "### Replay Buffer\n",
    "\n",
    "Not only use the data from the interaction between $\\pi$ and environment but also the previous data. All the data are maintained using a replay buffer, which is first in first out.\n",
    "\n",
    "$$\\bbox[#ffffbb]{\n",
    "\\left[\\begin{array}{cc}\n",
    "\\pi \\text{ interacts with }\\\\ \n",
    "\\text{the environment}\n",
    "\\end{array}\\right]}\n",
    "\\rightarrow \n",
    "\\underbrace{\\left[\n",
    "\\begin{array}{cc}\n",
    "\\vdots \\\\ \n",
    "\\bbox[#aaddaa]{\\text{ exp }} \\\\ \n",
    "\\bbox[#aaffaa]{\\text{ exp }} \\\\\n",
    "\\bbox[#ddaaaa]{\\text{ exp }} \\\\\n",
    "\\bbox[#ffaaaa]{\\text{ exp }} \\\\\n",
    "\\vdots\n",
    "\\end{array}\n",
    "\\right]}_{\\text{replay buffer}}\n",
    "\\xrightarrow[]{\\text{sample a batch}}\n",
    "\\bbox[#ddffdd]{\n",
    "\\Bigg[\\text{Learning }Q^{\\pi}(s, a)\\Bigg]\n",
    "}\n",
    "$$\n",
    "where the experience is denoted by $s_t, a_t, r_t, s_{t+1}$, is one interaction with the environment.\n",
    "\n",
    "### Typical Q-Learning Algorithm\n",
    "\n",
    "- Initialize Q-function $Q$, target Q-function $\\hat{Q} = Q$\n",
    "- In each episode\n",
    "  - For each time step $t$\n",
    "    1. Given state $s_t$, take action $a_t$ based on Q (epsilon greedy)\n",
    "    2. Obtain reward $r_t$, and reach new state $s_{t+1}$\n",
    "    3. Store $(s_t, a_t, r_t, s_{t+1})$ into buffer\n",
    "    4. Sample $(s_i, a_i, r_i, s_{i+1})$ from buffer (ususlly a batch)\n",
    "    5. Target $y = r_i+\\underset{a}{\\text{max}}\\hat{Q}(s_{i+1}, a)$\n",
    "    6. Update the parameters of $Q$ to make $Q(s_i, a_i)$ close to $y$ (regression)\n",
    "    7. Every $C$ steps reset $\\hat{Q} = Q$\n",
    "    \n",
    "    \n",
    "### Double DQN\n",
    "\n",
    "Q value is usually over-estimated, i.e. larger than the real reward.\n",
    "\n",
    "For training Q function, we always select the action that gives the maximal Q function. Moreover, the Q function is estimated using network which exist deviation with respected to the real Q function, which leads to the over estimation on Q value.\n",
    "\n",
    "$$Q(s_t, a_t) \\leftrightarrow r_t + \\underset{a}{\\text{max}} Q(s_{t+1}, a)$$\n",
    "\n",
    "In double DQN, we use two functions $Q$ and $Q'$. If $Q$ over-estimate $a$, so it is selected. $Q'$ would give it proper value. In contrast, the $Q'$ over-estimated actoin will not be selected by $Q$.\n",
    "\n",
    "$$Q(s_t, a_t) \\leftrightarrow r_t + Q'(s_{t+1}, arg\\underset{a}{\\text{max}}Q(s_{t+1}, a))$$\n",
    "\n",
    "\n",
    "### Dueling DQN\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{DQN:} &\\rightarrow &\\text{CNN} &\\rightarrow &\\text{FC} & &\\rightarrow &Q(s, a)\\\\\n",
    "\\text{Dueling DQN:} &\\rightarrow &\\text{CNN} &\\rightarrow &\\left\\{\\begin{array}{ll}\\text{FC}\\\\ \\text{FC}\\end{array}\\right. & \\left.\\begin{array}{ll}\\rightarrow\\text{scalar }V(s)\\\\ \\rightarrow\\text{Vector }A(s,a) \\end{array}\\right\\} &\\rightarrow &Q(s, a) = A(s,a)+V(s)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $V(s)$ in the structure can be viewed as biases of state. In general, when implementing dueling DQN, we would add a layer normalization after $A(s,a)$ such that constrain the layer $A(s,a)$ and let the network to be more constributed by the scaler $V(s)$.\n",
    "\n",
    "\n",
    "### Prioritized Reply\n",
    "\n",
    "The data with larger TD error, harder to train, in previous training has higher probability to be sampled.\n",
    "\n",
    "\n",
    "### Multi-step\n",
    "\n",
    "Balance between MC and TD.\n",
    "\n",
    "Saved trajectory.\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{MC:} &(s_t, a_t, r_t,\\cdots, s_{END-1}, a_{END-1}, r_{END-1}, s_{END} )\\\\\n",
    "\\text{TD:} &(s_t, a_t, r_t, s_{t+1})\\\\\n",
    "\\text{Multi-step:} &(s_t, a_t, r_t, \\cdots, s_{t+N}, a_{t+N}, r_{t+N}, s_{t+N+1}) \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Q function target\n",
    "\n",
    "$$Q(s_t, a_t) \\leftrightarrow \\sum_{t'=t}^{t+N} r_{t'}+\\hat{Q}(S_{t+N+1}, a_{t+N+1})$$\n",
    "\n",
    "### Noisy Net\n",
    "\n",
    "The preceding Epsilon Greedy is an approch that add noise on action\n",
    "\n",
    "$$a = \\left\\{\\begin{array}{ll}\n",
    "arg \\underset{a}{\\text{max}} Q(s, a) & \\text{with probability }1-\\epsilon\\\\\n",
    "random &\\text{otherwise}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "The Noisy Net approach inject noise into the parameters of Q-function at the beginning of each episode.\n",
    "$$a = arg \\underset{a}{\\text{max}} \\tilde{Q}(s, a) \\qquad \\text{where } Q(s,a) \\xrightarrow[]{\\text{Add Gaussian noise}} \\tilde{Q}(s,a)$$\n",
    "\n",
    "The noisy net is more commom on reality.\n",
    "\n",
    "- Noise on action. In one episode, given the same state, the agent may takes different actions.\n",
    "- Noise on parameter. In one episode, given the same(similar) state, the agent takes the same action, and try different action in the next episode. It is called state-dependent exploration, which explore in a consistent way.\n",
    "\n",
    "### Distributional Q-function\n",
    "\n",
    "$Q^{\\pi}(s, a)$ denotes the expectation of the cumulated reward after seeing observation $s$ and taking $a$ by the actor $\\pi$. However, different distributions may have the same expectation. This approach is proposed to output a distribution (histogram) from the network. Yet, this approach still select the action that gives the highest mean (expectation).\n",
    "\n",
    "### Rainbow\n",
    "\n",
    "Consist of all mentioned methods.\n",
    "\n",
    "\n",
    "# Dilemmas on Q-learning\n",
    "\n",
    "Action $a$ is a continuous vector. E.g. while driving a car, turn left and the degree.\n",
    "\n",
    "$$a = arg \\underset{a}{\\text{max}} Q(s, a)$$\n",
    "\n",
    "\n",
    "\n",
    "- Solution 1: Sample a set of actions: $\\{a_1, a_2, \\cdots , a_N\\}$.\n",
    "- Solution 2: Using gradient ascent to solve the optimization problem.\n",
    "- Solution 3: Design a network to make the optimization easy.\n",
    "$$s\\rightarrow \\underbrace{\\bbox[#ffcccc]{\\Bigg[Q^{\\pi}\\Bigg]}}_{\\text{network}}\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\mu(s) &\\text{vector}\\\\\n",
    "\\Sigma(s) &\\text{matrix}\\\\\n",
    "V(s) &\\text{scalar}\n",
    "\\end{array}\\right.$$\n",
    "where the output Q value is \n",
    "$$Q(s, a) = -(a - \\mu(s))^T\\Sigma(s)(a-\\mu(s))+V(s)$$\n",
    "the optimal value of $a$ is \n",
    "$$\\mu(s) = arg\\underset{a}{\\text{max}}Q(s, a)$$\n",
    "- Solution 4: Don't use Q-learning in continuous cases.\n",
    "\n",
    "# Actor-Critic\n",
    "\n",
    "Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "- Asynchronous: \n",
    "- Advantage: \n",
    "\n",
    "### Policy Gradient\n",
    "\n",
    "$$\\nabla \\bar{R}_{\\theta} \\approx \\frac{1}{N}\\sum_{n=1}^N\\sum_{t=1}^{T_n}\\underbrace{\\left(\\sum_{t'=t}^{T_n} \\gamma^{t'-t}r_{t'}^n - b\\right)}_{G_t^n\\text{: obtained via interaction }b\\text{: baseline}}\\nabla log p_{\\theta}(a_t^n|s_t^n)$$\n",
    "\n",
    "### Q-Learning\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
